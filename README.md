This repository showcases the implementation and comparison of two language models: a Bigram Language Model and a Decoder-Only Transformer. Both implementations are contained in separate Python files for simplicity, with bigram.py focusing on a probabilistic approach that predicts the next character based on the previous one, and transformer.py implementing a modern self-attention-based architecture to model long-range dependencies effectively. The models are trained and evaluated using a character-level tokenized dataset of Shakespeare's complete works (input.txt). To set up, clone the repository, and ensure the dataset is available in the data/ folder. Training can be performed by running python bigram.py for the Bigram Language Model and python transformer.py for the Decoder-Only Transformer. Results demonstrate that while the Bigram Model is lightweight and fast, it struggles with long-range dependencies, whereas the Transformer generates more coherent and contextually relevant text. Future enhancements could include hyperparameter tuning, experimenting with alternative tokenization approaches, or visualizing the Transformerâ€™s attention weights. 
